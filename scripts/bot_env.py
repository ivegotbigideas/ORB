"""
Adapted from template_my_robot_env.py in openai_ros.
"""

import math
import numpy as np
from openai_ros import robot_gazebo_env
import api
from api import Orb, Target
import rospy


class BotEnv(robot_gazebo_env.RobotGazeboEnv):
    """
    Superclass for all Robot environments.
    """

    def __init__(self):
        """
        Initializes a new Robot environment.
        """
        # Variables that we give through the constructor.

        # TODO Not sure what these are for
        # Internal Vars
        # print("START init bot_env")
        self.controllers_list = []

        self.robot_name_space = ""

        reset_controls_bool = False

        # We launch the init function of the Parent Class robot_gazebo_env.RobotGazeboEnv

        super(BotEnv, self).__init__(
            controllers_list=self.controllers_list,
            robot_name_space=self.robot_name_space,
            reset_controls=reset_controls_bool,
        )

        self.bot_api = Orb()
        self.target_api = Target()

        self.SKIP = 10
        self.MAX_STEPS = 25
        self.steps = 0
        self.grid_squares = 0
        self.previous_dist = 100000
        # print("END init bot_env")

    def step(self, action):
        """
        Function executed each time step.
        Here we get the action execute it in a time step and retrieve the
        observations generated by that action.
        :param action:
        :return: obs, reward, done, info
        """

        """
        Here we should convert the action num to movement action, execute the action in the
        simulation and get the observations result of performing that action.
        """

        self.gazebo.unpauseSim()
        self._set_action(action)
        rate = rospy.Rate(1 / self.SKIP)
        rate.sleep()
        self.gazebo.pauseSim()
        obs = self._get_obs()
        done = self._is_done(obs)
        info = {}
        reward = self._compute_reward(obs, done)
        self.cumulated_episode_reward += reward

        return obs, reward, done, info

    # Methods needed by the RobotGazeboEnv
    # ----------------------------
    def _check_all_systems_ready(self):
        """
        Checks that all the sensors, publishers and other simulation systems are
        operational.
        """
        # TODO Not sure what to do here
        return True

    # Methods that the TrainingEnvironment will need to define here as virtual
    # because they will be used in RobotGazeboEnv GrandParentClass and defined in the
    # TrainingEnvironment.
    # ----------------------------
    def _set_init_pose(self):
        """
        Sets the Robot in its init pose.
        """
        while True:
            self.previous_dist = 100000
            self.bot_api.randomise_robot_pose()
            self.target_api.randomise_target_pose()
            
            if not self._is_touching_target():
                break
    
    def _init_env_variables(self):
        """
        Inits variables needed to be initialised each time we reset at the start
        of an episode.
        """
        self.steps = 0
        self.grid_squares = 0
        self._set_init_pose()

    def _compute_reward(self, observations, done):
        """
        Calculates the reward to give based on the observations given.
        """
        reward = 0

        # High reward for reaching the target
        if self._is_touching_target():
            reward += 10000
            return reward

        # Reward for map exploration
        map_list = self.bot_api.get_latest_slam_map()["data"]
        next_grid_squares = sum(1 for i in map_list if i == 1)

        if next_grid_squares > self.grid_squares:
            self.grid_squares = next_grid_squares
            reward += 10  # Adjusted reward for exploration

        # Calculate distance to the target
        new_dist = self._distance_to_target()
        
        if(self.previous_dist < 1000):
            # Adjusted reward/penalty for distance
            distance_change = self.previous_dist - new_dist
            reward += distance_change * 100  # Scale reward to be sensitive to small changes

        reward -= 1
        self.previous_dist = new_dist

        print(reward)
        return reward


    def _set_action(self, action):
        """
        Applies the given action to the simulation.
        """
        if action == 0:
            act_string = "f"
        elif action == 1:
            act_string = "b"
        elif action == 2:
            act_string = "cw"
        elif action == 3:
            act_string = "acw"
        else:
            act_string = "stop"

        self.bot_api.move_robot(act_string)

    def _get_obs(self):
        # print("OBS get_latest_camera")
        camera_data = self.bot_api.get_latest_camera_data()
        camera_array = (np.array(camera_data)).T.flatten()

        # print("OBS get_latest_lidar")
        self.gazebo.unpauseSim()
        lidar_data = self.bot_api.get_latest_lidar_data()
        self.gazebo.pauseSim()
        lidar_array = np.array(lidar_data["ranges"])

        # print("OBS concat")
        return np.concatenate((camera_array, lidar_array))

    def _is_done(self, observations):
        """
        Checks if episode done based on observations given.
        """
        self.steps += 1
        return self.steps >= self.MAX_STEPS or self._is_touching_target()

    # Methods that the TrainingEnvironment will need.
    # ----------------------------

    def _distance_to_target(self):
        # print("TOUCHING robot_truth")
        self.gazebo.unpauseSim()
        bot_pose = self.bot_api.get_ground_truth_robot_pose()
        self.gazebo.pauseSim()
        bot_x = bot_pose["position"]["x"]
        bot_z = bot_pose["position"]["z"]

        # print("TOUCHING target_truth")
        self.gazebo.unpauseSim()
        target_pose = self.target_api.get_ground_truth_target_pose()
        self.gazebo.pauseSim()
        target_x = target_pose["position"]["x"]
        target_z = target_pose["position"]["z"]

        x_dist = bot_x - target_x
        z_dist = bot_z - target_z
        dist = math.sqrt((x_dist**2) + (z_dist**2))
        return dist

    def _is_touching_target(self):
        """
        Returns True if the robot is within 1 co-ordinate point of the target.
        """
        # print("RETURNING")
        return self._distance_to_target() <= 1

    def set_skip(self, newSkip):
        SKIP = newSkip